{"cells":[{"cell_type":"code","execution_count":1,"id":"252717f4","metadata":{"id":"252717f4","executionInfo":{"status":"ok","timestamp":1667661177615,"user_tz":-420,"elapsed":900,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["import re\n","import math\n","import random\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54gMKrknb9XN","executionInfo":{"status":"ok","timestamp":1667661205432,"user_tz":-420,"elapsed":21105,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}},"outputId":"1f29f795-b314-4ca8-9024-c4d69a7f1654"},"id":"54gMKrknb9XN","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"id":"6745bc59","metadata":{"id":"6745bc59","executionInfo":{"status":"ok","timestamp":1667661792633,"user_tz":-420,"elapsed":272,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def get_data_sentences():\n","\n","    with open(r'/content/drive/MyDrive/Colab Notebooks/clean_data.txt', 'r+', encoding='utf-8') as file:\n","        data = file.readlines()\n","        file.close()\n","        \n","    return data"]},{"cell_type":"code","execution_count":4,"id":"0481ed6e","metadata":{"id":"0481ed6e","executionInfo":{"status":"ok","timestamp":1667661794517,"user_tz":-420,"elapsed":291,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def get_tokenized_data(sentences):\n","\n","    tokenized_data = []\n","\n","    for sentence in sentences:\n","        tokenized_data.append(sentence.split(' '))\n","        \n","    return tokenized_data"]},{"cell_type":"code","execution_count":5,"id":"bf4124e2","metadata":{"id":"bf4124e2","executionInfo":{"status":"ok","timestamp":1667661795103,"user_tz":-420,"elapsed":288,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def count_words(tokenized_sentences):   \n","\n","    word_counts = {}\n","\n","    for sentence in tokenized_sentences:\n","\n","        for token in sentence:\n","\n","            if token not in word_counts:\n","                word_counts[token] = 1\n","            else:\n","                word_counts[token] += 1\n","                \n","    return word_counts"]},{"cell_type":"code","execution_count":6,"id":"49b47824","metadata":{"id":"49b47824","executionInfo":{"status":"ok","timestamp":1667661797042,"user_tz":-420,"elapsed":279,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n","\n","    closed_vocab = []\n","    word_counts = count_words(tokenized_sentences)\n","\n","    for word, cnt in word_counts.items():\n","\n","        if cnt >= count_threshold:\n","            closed_vocab.append(word)\n","            \n","    return closed_vocab"]},{"cell_type":"code","execution_count":37,"id":"c1939652","metadata":{"id":"c1939652","executionInfo":{"status":"ok","timestamp":1667663368368,"user_tz":-420,"elapsed":336,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n","\n","    vocabulary = set(vocabulary)\n","    new_tokenized_sentences = []\n","\n","    for sentence in tokenized_sentences:\n","\n","        new_sentence = []\n","        for token in sentence:\n","\n","            if token in vocabulary:\n","\n","                new_sentence.append(token)\n","\n","            else:\n","\n","                new_sentence.append(unknown_token)\n","\n","        new_tokenized_sentences.append(new_sentence)\n","\n","    return new_tokenized_sentences"]},{"cell_type":"code","execution_count":38,"id":"04489633","metadata":{"id":"04489633","executionInfo":{"status":"ok","timestamp":1667663420825,"user_tz":-420,"elapsed":545,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def preprocess_data(tokenized_data, count_threshold=1, unknown_token=\"<unk>\"):\n","\n","    vocabulary = get_words_with_nplus_frequency(tokenized_data, count_threshold)\n","\n","    train_data = replace_oov_words_by_unk(tokenized_data, vocabulary, unknown_token)\n","\n","    return train_data, vocabulary"]},{"cell_type":"code","execution_count":9,"id":"dccb4e32","metadata":{"id":"dccb4e32","executionInfo":{"status":"ok","timestamp":1667661802623,"user_tz":-420,"elapsed":2,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n","\n","    n_grams = {}\n","\n","    for sentence in data:\n","\n","        sentence = [start_token]*n + sentence + [end_token]\n","        sentence = tuple(sentence)\n","        m = len(sentence) if n == 1 else len(sentence) - n + 1\n","\n","        for i in range(m): \n","\n","            n_gram = sentence[i:i+n]\n","            if n_gram in n_grams.keys():\n","                n_grams[n_gram] += 1\n","            else:\n","                n_grams[n_gram] = 1\n","                \n","    return n_grams"]},{"cell_type":"code","execution_count":10,"id":"6b1368e0","metadata":{"id":"6b1368e0","executionInfo":{"status":"ok","timestamp":1667661805086,"user_tz":-420,"elapsed":277,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1):\n","\n","    previous_n_gram = tuple(previous_n_gram)\n","    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n","\n","    denominator = previous_n_gram_count + k * vocabulary_size\n","\n","    n_plus1_gram = previous_n_gram + (word,) \n","    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n","\n","    numerator = n_plus1_gram_count + k\n","\n","    probability = numerator / denominator\n","\n","    return probability"]},{"cell_type":"code","execution_count":11,"id":"45ba6383","metadata":{"id":"45ba6383","executionInfo":{"status":"ok","timestamp":1667661806293,"user_tz":-420,"elapsed":3,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1):\n","\n","    previous_n_gram = tuple(previous_n_gram)    \n","    vocabulary = vocabulary + [end_token, unknown_token]    \n","    vocabulary_size = len(vocabulary)   \n","\n","    probabilities = {}\n","\n","    for word in vocabulary:\n","\n","        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n","        probabilities[word] = probability\n","\n","    return probabilities"]},{"cell_type":"code","execution_count":12,"id":"560e3025","metadata":{"id":"560e3025","executionInfo":{"status":"ok","timestamp":1667661809220,"user_tz":-420,"elapsed":286,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1):\n","\n","    n = len(list(n_gram_counts.keys())[0]) \n","\n","    sentence = [start_token] * n + sentence + [end_token]\n","    sentence = tuple(sentence)\n","\n","    N = len(sentence)\n","    product_pi = 1.0\n","\n","    for t in range(n, N):\n","\n","        n_gram = sentence[t-n:t]\n","        word = sentence[t]\n","        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n","        product_pi *= (1 / probability)\n","\n","    perplexity = (product_pi)**(1/N)\n","    \n","    return perplexity"]},{"cell_type":"code","execution_count":13,"id":"3a494f87","metadata":{"id":"3a494f87","executionInfo":{"status":"ok","timestamp":1667661818126,"user_tz":-420,"elapsed":289,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1, start_with=None):\n","\n","    n = len(list(n_gram_counts.keys())[0]) \n","\n","    previous_n_gram = previous_tokens[-n:]\n","    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n","\n","    suggestion = None\n","    max_prob = 0\n","\n","    for word, prob in probabilities.items():\n","\n","        if start_with:\n","            if not word.startswith(start_with):\n","                continue  \n","                \n","        if prob > max_prob:\n","            suggestion = word\n","            max_prob = prob\n","\n","    return suggestion, max_prob"]},{"cell_type":"code","execution_count":14,"id":"5f6d488e","metadata":{"id":"5f6d488e","executionInfo":{"status":"ok","timestamp":1667661820835,"user_tz":-420,"elapsed":297,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[],"source":["def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1, start_with=None):\n","\n","    model_counts = len(n_gram_counts_list)\n","    suggestions = []\n","\n","    for i in range(model_counts-1):\n","\n","        n_gram_counts = n_gram_counts_list[i]\n","        n_plus1_gram_counts = n_gram_counts_list[i+1]\n","        \n","        suggestion = suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k, start_with=start_with)\n","        suggestions.append(suggestion)\n","\n","    return suggestions"]},{"cell_type":"code","source":["def print_suggetions(suggestions):\n","\n","  print(\"Suggestion: \")\n","\n","  for word in suggestions:\n","    \n","    print(word)"],"metadata":{"id":"QfDjS3FxbKZC","executionInfo":{"status":"ok","timestamp":1667662496080,"user_tz":-420,"elapsed":413,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"id":"QfDjS3FxbKZC","execution_count":26,"outputs":[]},{"cell_type":"code","source":["data = get_data_sentences()\n","tokenized_data = get_tokenized_data(data)\n","\n","random.shuffle(tokenized_data)\n","\n","train_data, vocabulary = preprocess_data(tokenized_data, 10)"],"metadata":{"id":"7Oz9Y4rHYnUi","executionInfo":{"status":"ok","timestamp":1667663464425,"user_tz":-420,"elapsed":9209,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"id":"7Oz9Y4rHYnUi","execution_count":40,"outputs":[]},{"cell_type":"code","execution_count":21,"id":"9b098f6a","metadata":{"id":"9b098f6a","outputId":"19e33b58-8e63-460d-b1b6-45068ff0f688","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667662194090,"user_tz":-420,"elapsed":45724,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["n-grams = 1\n","n-grams = 2\n","n-grams = 3\n","n-grams = 4\n","n-grams = 5\n","n-grams = 6\n"]}],"source":["n_gram_counts_list = []\n","\n","for n in range(1, 7):\n","\n","    print(\"n-grams =\", n)\n","    \n","    n_model_counts = count_n_grams(train_data, n)\n","    n_gram_counts_list.append(n_model_counts)"]},{"cell_type":"code","execution_count":28,"id":"4e48da5a","metadata":{"id":"4e48da5a","outputId":"e07dd54d-410c-429a-ea96-6e92832ce0a5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667662560039,"user_tz":-420,"elapsed":581,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous words: ['đường', 'ống', 'nord']\n","Suggestion: \n","('stream', 0.046193835870776086)\n","('stream', 0.018302956042272937)\n","('stream', 0.018227770544535497)\n","Perplexity with n-grams = 1: 134.43086588964763\n","Perplexity with n-grams = 2: 113.06874118862112\n","Perplexity with n-grams = 3: 125.05429531292415\n"]}],"source":["tokens_1 = [\"đường\", \"ống\", \"nord\"]\n","suggest_1 = get_suggestions(tokens_1, n_gram_counts_list[:len(tokens_1) + 1], vocabulary, k=1)\n","\n","print(\"Previous words:\", tokens_1)\n","print_suggetions(suggest_1)\n","\n","for n in range(len(tokens_1)):\n","  \n","  perplexity = calculate_perplexity(tokens_1, n_gram_counts_list[n], n_gram_counts_list[n + 1], len(vocabulary), k=1)\n","\n","  print(f\"Perplexity with n-grams = {n + 1}: {perplexity}\")"]},{"cell_type":"code","execution_count":29,"id":"2db6063d","metadata":{"id":"2db6063d","outputId":"a36820a9-4f4c-46bb-a338-4125b633a871","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667662589635,"user_tz":-420,"elapsed":452,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous words: ['vũ', 'khí']\n","Suggestion: \n","('đốt', 0.0658965896589659)\n","('hạt', 0.026947984123668268)\n","Perplexity with n-grams = 0: 178.29285477102198\n","Perplexity with n-grams = 1: 132.80030280497326\n"]}],"source":["tokens_2 = [\"vũ\", \"khí\"]\n","suggest_2 = get_suggestions(tokens_2, n_gram_counts_list[:len(tokens_2) + 1], vocabulary, k=1)\n","\n","print(\"Previous words:\", tokens_2)\n","print_suggetions(suggest_2)\n","\n","for n in range(len(tokens_2)):\n","  \n","  perplexity = calculate_perplexity(tokens_2, n_gram_counts_list[n], n_gram_counts_list[n + 1], len(vocabulary), k=1)\n","\n","  print(f\"Perplexity with n-grams = {n + 1}: {perplexity}\")"]},{"cell_type":"code","source":["tokens_3 = [\"từ\", \"trường\"]\n","suggest_3 = get_suggestions(tokens_3, n_gram_counts_list[:len(tokens_3) + 1], vocabulary, k=1)\n","\n","print(\"Previous words:\", tokens_3)\n","print_suggetions(suggest_3)\n","\n","for n in range(len(tokens_3)):\n","  \n","  perplexity = calculate_perplexity(tokens_3, n_gram_counts_list[n], n_gram_counts_list[n + 1], len(vocabulary), k=1)\n","\n","  print(f\"Perplexity with n-grams = {n + 1}: {perplexity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgWo6zHzFpN_","executionInfo":{"status":"ok","timestamp":1667662653738,"user_tz":-420,"elapsed":293,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}},"outputId":"b543e9ea-7a30-4fce-81e7-6021d3972f1f"},"id":"LgWo6zHzFpN_","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous words: ['từ', 'trường']\n","Suggestion: \n","('hợp', 0.05799509156126109)\n","('trái', 0.0019337871287128713)\n","Perplexity with n-grams = 1: 202.51201140107563\n","Perplexity with n-grams = 2: 95.38858738420113\n"]}]},{"cell_type":"code","source":["tokens_4 = [\"thổ\", 'nhĩ']\n","suggest_4 = get_suggestions(tokens_4, n_gram_counts_list[:len(tokens_4) + 1], vocabulary, k=1)\n","\n","print(\"Previous words:\", tokens_4)\n","print_suggetions(suggest_4)\n","\n","for n in range(len(tokens_4)):\n","  \n","  perplexity = calculate_perplexity(tokens_4, n_gram_counts_list[n], n_gram_counts_list[n + 1], len(vocabulary), k=1)\n","\n","  print(f\"Perplexity with n-grams = {n + 1}: {perplexity}\")"],"metadata":{"id":"PbplEWwJGJ9H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667662700804,"user_tz":-420,"elapsed":276,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}},"outputId":"328fc912-9309-48d6-d1b9-129d15cf112b"},"id":"PbplEWwJGJ9H","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous words: ['thổ', 'nhĩ']\n","Suggestion: \n","('kỳ', 0.08266685540377947)\n","('kỳ', 0.08010910918096333)\n","Perplexity with n-grams = 1: 193.95820780900857\n","Perplexity with n-grams = 2: 136.97050638722203\n"]}]},{"cell_type":"code","source":["tokens_5 = [\"nơi\", \"dễ\", \"tổn\"]\n","suggest_5 = get_suggestions(tokens_5, n_gram_counts_list[:len(tokens_5) + 1], vocabulary, k=1)\n","\n","print(\"Previous words:\", tokens_5)\n","print_suggetions(suggest_5)\n","\n","for n in range(len(tokens_5)):\n","  \n","  perplexity = calculate_perplexity(tokens_5, n_gram_counts_list[n], n_gram_counts_list[n + 1], len(vocabulary), k=1)\n","\n","  print(f\"Perplexity with n-grams = {n + 1}: {perplexity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYHlQetZGub5","executionInfo":{"status":"ok","timestamp":1667662792879,"user_tz":-420,"elapsed":461,"user":{"displayName":"Snakesss","userId":"11156291641231234234"}},"outputId":"737c3ff2-30d4-49d2-8c6c-e9193ce9da06"},"id":"hYHlQetZGub5","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous words: ['nơi', 'dễ', 'tổn']\n","Suggestion: \n","('thương', 0.02998016018811081)\n","('thương', 0.001713796058269066)\n","('trong', 7.803964413922273e-05)\n","Perplexity with n-grams = 1: 606.9389615801686\n","Perplexity with n-grams = 2: 416.3179354514162\n","Perplexity with n-grams = 3: 175.82915457033627\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}